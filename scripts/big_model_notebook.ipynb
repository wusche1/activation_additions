{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Script reimplementing activation additions in torch, for bigger language models.\n",
    "\n",
    "Qualitatively, works for the full Vicuna series (up to 33B), and for local LLaMA\n",
    "models (up to 65B). Note that, quantitatively, logits diverge from the original\n",
    "implementationâ€”possibly due to the original's support for positional addition,\n",
    "padding, etc. See scripts/implementations_comparison.py\n",
    "\"\"\"\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import accelerate\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR: str = \"/mnt/ssd-2/mesaoptimizer/llama/hf/65B\"\n",
    "MAX_NEW_TOKENS: int = 50\n",
    "NUM_CONTINUATIONS: int = 5\n",
    "SEED: int = 0\n",
    "DO_SAMPLE: bool = True\n",
    "TEMPERATURE: float = 1.0\n",
    "TOP_P: float = 0.9\n",
    "REP_PENALTY: float = 2.0\n",
    "PLUS_PROMPT, MINUS_PROMPT = \"Harry Potter \", \" Lord of the Rings\"\n",
    "CHAT_PROMPT: str = \"My favourite book is \"\n",
    "ACT_NUM: int = 6\n",
    "COEFF: int = 4\n",
    "\n",
    "sampling_kwargs: dict = {\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"top_p\": TOP_P,\n",
    "    \"repetition_penalty\": REP_PENALTY,\n",
    "}\n",
    "\n",
    "# Set torch and numpy seeds.\n",
    "t.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b639ec79a64b6dbef1857472eec152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (32): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (33): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (34): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (35): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (36): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (37): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (38): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (39): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (40): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (41): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (42): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (43): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (44): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (45): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (46): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (47): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (48): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (49): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (50): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (51): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (52): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (53): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (54): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (55): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (56): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (57): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (58): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (59): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (60): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (61): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (62): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (63): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (64): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (65): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (66): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (67): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (68): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (69): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (70): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (71): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (72): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (73): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (74): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (75): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (76): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (77): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (78): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (79): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "# An accelerate wrapper does all the parallelization across devices.\n",
    "accelerator = accelerate.Accelerator()\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_DIR, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_DIR)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "model.tie_weights()\n",
    "# model.half()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'activation_additions.adhoc_actadds' from '/mnt/ssd-2/mesaoptimizer/wuschel/new_download/activation_additions/activation_additions/adhoc_actadds.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import activation_additions.adhoc_actadds as adhoc_actadds\n",
    "importlib.reload(adhoc_actadds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_conspiracies_qa = [\n",
    "    {\"question\": \"Is the earth flat?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is Pluto a planet?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Was Napoleon Bonaparte extremely short?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Can you see the Great Wall of China from space with the naked eye?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do humans only use 10% of their brains?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Does shaving cause hair to grow back thicker?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is MSG (monosodium glutamate) harmful to health?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do goldfish have a three-second memory?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are bats blind?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is it dangerous to touch baby birds, as the mother will reject them due to the human scent?\", \"answer\": \"No\"}\n",
    "    {\"question\": \"Are vaccines responsible for causing autism?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Was the moon landing in 1969 faked by the U.S. government?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are chemtrails real and being used to control the population?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Did the ancient Egyptians use aliens to build the pyramids?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is the Bermuda Triangle responsible for numerous supernatural disappearances?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Did the U.S. government hide the existence of mermaids?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is there a secret underground city beneath the Denver International Airport?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is Bigfoot a real creature that has been encountered by many?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do lizard people control the world's governments?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Did the Titanic never sink and was instead switched with its sister ship, the Olympic?\", \"answer\": \"No\"}\n",
    "]\n",
    "true_conspiracies_qa = [\n",
    "    {\"question\": \"Was the U.S. government involved in a secret mind-control experiment called MKUltra?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government have a plan called Operation Northwoods, which proposed committing acts of terrorism on its own soil to justify war with Cuba?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there an actual 'poisoned' alcohol program during the Prohibition era where the U.S. government deliberately tainted industrial alcohols to deter consumption?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government secretly dose its citizens with LSD in the 1950s and 1960s?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there an operation called Operation Paperclip where the U.S. recruited former Nazi scientists after World War II?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government conduct syphilis experiments on unsuspecting Guatemalan citizens in the 1940s?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is it true that the U.S. Public Health Service conducted a study on black men in Tuskegee, Alabama, allowing them to die from untreated syphilis?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the CIA fund and support modern art during the Cold War as a form of cultural diplomacy?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there a genuine concern in the 1960s about a 'gay bomb' - a non-lethal bomb that would release chemicals to make enemy troops attracted to each other?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the British intelligence agency MI6 once use semen as invisible ink?\", \"answer\": \"Yes\"}\n",
    "    {\"question\": \"Was there a U.S. government program called Operation CHAOS that spied on anti-war activists?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the FBI have a program called COINTELPRO to surveil, infiltrate, and disrupt American political organizations?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was Agent Orange, a herbicide, used by the U.S. military during the Vietnam War and later caused health issues?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government once infect San Francisco with bacteria to test biowarfare capabilities?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was the Gulf of Tonkin incident exaggerated to justify U.S. involvement in Vietnam?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government approve a project named Project Sunshine which took human body parts to test them for radiation?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the 'Heart Attack Gun' a real weapon developed by the CIA?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the British government conduct germ warfare tests on its own public during the 20th century?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. Navy project sounds at 135+ decibels during the 'whale wars' in the Bahamas, leading to mass strandings?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did Operation Gladio involve secret NATO armies that could be activated in the event of a communist takeover?\", \"answer\": \"Yes\"}\n",
    "]\n",
    "\n",
    "true_plausible_qa = [\n",
    "    {\"question\": \"Do plants use photosynthesis to produce energy?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is water composed of hydrogen and oxygen?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was Albert Einstein famous for his theory of relativity?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Can a solar eclipse occur only during a new moon?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the chemical symbol for water Hâ‚‚O?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the sun a star?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Do mammals have three middle ear bones?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is DNA a double helix structure?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Does Earth have one natural satellite, known as the Moon?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is oxygen necessary for human respiration?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Do honeybees communicate with each other through a 'waggle dance'?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the periodic table organized based on atomic number?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Can tsunamis be caused by underwater earthquakes?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is our galaxy known as the Milky Way?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the process by which a caterpillar transforms into a butterfly called metamorphosis?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is carbon dioxide considered a greenhouse gas?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Do humans have 23 pairs of chromosomes?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the boiling point of water at 1 atm pressure 100Â°C?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the primary purpose of white blood cells to fight infections?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Does salt consist of sodium and chloride ions?\", \"answer\": \"Yes\"}\n",
    "]\n",
    "\n",
    "false_plausible_qa = [\n",
    "    {\"question\": \"Is diamond primarily made out of nitrogen?\", \"answer\": \"No\"},  # Diamonds are primarily carbon.\n",
    "    {\"question\": \"Is the primary ingredient in glass liquid sand?\", \"answer\": \"No\"},  # Glass is made from melted silica, but it's not 'liquid sand'.\n",
    "    {\"question\": \"Do penguins primarily live in the Arctic?\", \"answer\": \"No\"},  # They live in the Southern Hemisphere, primarily Antarctica.\n",
    "    {\"question\": \"Is the primary component of human blood blue oxygen?\", \"answer\": \"No\"},  # Blood is never blue; it's red due to hemoglobin.\n",
    "    {\"question\": \"Are polar bears predominantly left-handed?\", \"answer\": \"No\"},  # There's no scientific evidence to support this.\n",
    "    {\"question\": \"Is the Sahara Desert the largest desert in the world?\", \"answer\": \"No\"},  # Antarctica is technically the largest desert.\n",
    "    {\"question\": \"Do bats primarily navigate using their eyes?\", \"answer\": \"No\"},  # Bats predominantly use echolocation.\n",
    "    {\"question\": \"Is the sun yellow because it's made of burning sulfur?\", \"answer\": \"No\"},  # The sun's color comes from the temperature of its surface and not from burning sulfur.\n",
    "    {\"question\": \"Is the primary purpose of the Great Wall of China to keep out rabbits?\", \"answer\": \"No\"},  # It was primarily for defense against invasions.\n",
    "    {\"question\": \"Are rainbows circular because the raindrops are square?\", \"answer\": \"No\"}  # Rainbows are circular due to the refraction and reflection of light in raindrops, which are spherical.\n",
    "    {\"question\": \"Is Mars primarily blue because of its vast oceans?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is chocolate made from beans of the cocoa palm tree?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Does thunder originate from clouds colliding?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is Venus the closest planet to the sun?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are human teeth considered bones?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do polar bears primarily eat penguins?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are black holes flat disks that rotate?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is the main gas in Earth's atmosphere nitrogen trifluoride?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do apples grow primarily on vines?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is lightning a result of positive charges moving from the ground to clouds?\", \"answer\": \"No\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vec = adhoc_actadds.SteeringVector()\n",
    "layer=50\n",
    "coeff=2\n",
    "steering_vec.add_entry(\" Bush did 9/11\", layer, 1*coeff)\n",
    "steering_vec.add_entry(\" al quaida did 9/11 \", layer, -1*coeff)\n",
    "\n",
    "prompt=\"Question: Did the gouvernment do 9/11? Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Bush did 9/11, Coefficient: 2\n",
      "Prompt:  al quaida did 9/11 , Coefficient: -2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "steered_strings=adhoc_actadds.gen_with_steering(model,tokenizer, prompt, steering_vec.vector,MAX_NEW_TOKENS=5)\n",
    "unsteered_stings=adhoc_actadds.gen_without_steering(model,tokenizer, prompt,MAX_NEW_TOKENS=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Question: Did the gouvernment do 9/11? Answer: yes\n",
      "Question- did\n",
      "<s>Question: Did the gouvernment do 9/11? Answer:\n",
      "Written by J\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: We did it. And\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: The question is impossible to\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: No, but there are\n",
      "-----------\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: Not likely!\n",
      "I\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: Yes they did.\n",
      "\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: Yes.\n",
      "Tod\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: No.\n",
      "Questioner\n",
      "<s>Question: Did the gouvernment do 9/11? Answer: I hope you're\n"
     ]
    }
   ],
   "source": [
    "for s in steered_strings:\n",
    "    print(s)\n",
    "print(\"-----------\")\n",
    "for s in unsteered_stings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get the steering vector.\n",
    "plus_activation = get_resid_pre(PLUS_PROMPT, ACT_NUM)\n",
    "minus_activation = get_resid_pre(MINUS_PROMPT, ACT_NUM)\n",
    "plus_activation, minus_activation=adhoc_actadds.resize_tensors(plus_activation, minus_activation)\n",
    "assert plus_activation.shape == minus_activation.shape \n",
    "steering_vec = plus_activation - minus_activation\n",
    "\n",
    "#steering_vec = resize_tensor(plus_activation ,- minus_activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Example Prompt', 'layer': 1, 'coefficient': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "steering_vec = adhoc_actadds.SteeringVector()\n",
    "steering_vec.add_entry(\"Example Prompt\", 1, 0.5)\n",
    "print(steering_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1713039377.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for steering_vector_dict in steering_vec:\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def gen_with_steering(model, prompt, steering_vec_dict):\n",
    "    activations = []\n",
    "    \n",
    "    # Check whether all layers are the same\n",
    "    unique_layers = set(item[\"layer\"] for item in steering_vec_dict)\n",
    "    \n",
    "    if len(unique_layers) > 1:\n",
    "        raise ValueError(\"All layers in steering_vec_dict must be the same.\")\n",
    "    \n",
    "    # Extract the common layer value\n",
    "    layer = unique_layers.pop()\n",
    "\n",
    "    for steering_vector_dict in steering_vec_dict:\n",
    "        # Extract values from the dictionary\n",
    "        prompt = steering_vector_dict[\"prompt\"]\n",
    "        coeff = steering_vector_dict[\"coefficient\"]\n",
    "        \n",
    "        # Compute activations using the given prompt and layer\n",
    "        activations.append(get_resid_pre(prompt, layer) * coeff)\n",
    "    # Return the sum of all activations\n",
    "    activations=resize_tensors(*activations)\n",
    "    steering_vec= sum(activations)\n",
    "    # %%\n",
    "    # Run the model with the steering vector * COEFF.\n",
    "    def _steering_hook(_, inpt):\n",
    "        (resid_pre,) = inpt\n",
    "        # Only add to the first forward-pass, not to later tokens.\n",
    "        if resid_pre.shape[1] == 1:\n",
    "            # Caching in `model.generate` for new tokens.\n",
    "            return\n",
    "        ppos, apos = resid_pre.shape[1], steering_vec.shape[1]\n",
    "        assert (\n",
    "            apos <= ppos\n",
    "        ), f\"More modified streams ({apos}) than prompt streams ({ppos})!\"\n",
    "        resid_pre[:, :apos, :] += COEFF * steering_vec\n",
    "    \n",
    "    layer_name = get_blocks(model)[layer]\n",
    "    with pre_hooks(hooks=[(layer_name, _steering_hook)]):\n",
    "        steered_tokens = accelerator.unwrap_model(\n",
    "            model.generate(\n",
    "                **tokenize([CHAT_PROMPT] * NUM_CONTINUATIONS),\n",
    "                generation_config=GenerationConfig(\n",
    "                    **sampling_kwargs,\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    steered_strings = [tokenizer.decode(o) for o in steered_tokens]\n",
    "    return steered_strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> My favourite book is 20 years old today!\n",
      "This entry was posted in Writing on June,16 by Twisted Jellyfish.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "--------------------------------------------------------------------------------\n",
      "<s>My favourite book is 20 years old today â€“ so what have the stars done since?\n",
      "The first Harry Potter novel was released on June, Monday (Photo: Amazon) A lot of time has passed by! Itâ€™s hard to believe that it all began\n",
      "--------------------------------------------------------------------------------\n",
      "<s> My favourite book is 20th on the list of most read books in schools.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "--------------------------------------------------------------------------------\n",
      "<s>My favourite book is 20 years old\n",
      "Posted in: FILM, FOOD & DRINK.\tTagged:#PotterHeads #Harry Potters Birthday Cake#BakerdaysCakesLondon @har\n",
      "--------------------------------------------------------------------------------\n",
      "<s>My favourite book is 20 years old.\n",
      "A lot of people have grown up with the young wizard, as theyâ€™ve read all seven books and seen eight movies over two decades â€” many who weren't even alive when author JK Row\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Run the model with the steering vector * COEFF.\n",
    "def _steering_hook(_, inpt):\n",
    "    (resid_pre,) = inpt\n",
    "    # Only add to the first forward-pass, not to later tokens.\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        # Caching in `model.generate` for new tokens.\n",
    "        return\n",
    "    ppos, apos = resid_pre.shape[1], steering_vec.shape[1]\n",
    "    assert (\n",
    "        apos <= ppos\n",
    "    ), f\"More modified streams ({apos}) than prompt streams ({ppos})!\"\n",
    "    resid_pre[:, :apos, :] += COEFF * steering_vec\n",
    "\n",
    "\n",
    "layer = get_blocks(model)[ACT_NUM]\n",
    "with pre_hooks(hooks=[(layer, _steering_hook)]):\n",
    "    steered_tokens = accelerator.unwrap_model(\n",
    "        model.generate(\n",
    "            **tokenize([CHAT_PROMPT] * NUM_CONTINUATIONS),\n",
    "            generation_config=GenerationConfig(\n",
    "                **sampling_kwargs,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "steered_strings = [tokenizer.decode(o) for o in steered_tokens]\n",
    "print((\"\\n\" + \"-\" * 80 + \"\\n\").join(steered_strings))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wuschel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
