{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Script reimplementing activation additions in torch, for bigger language models.\n",
    "\n",
    "Qualitatively, works for the full Vicuna series (up to 33B), and for local LLaMA\n",
    "models (up to 65B). Note that, quantitatively, logits diverge from the original\n",
    "implementationâ€”possibly due to the original's support for positional addition,\n",
    "padding, etc. See scripts/implementations_comparison.py\n",
    "\"\"\"\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import accelerate\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR: str = \"/mnt/ssd-2/mesaoptimizer/llama/hf/7B\"\n",
    "MAX_NEW_TOKENS: int = 50\n",
    "NUM_CONTINUATIONS: int = 5\n",
    "SEED: int = 0\n",
    "DO_SAMPLE: bool = True\n",
    "TEMPERATURE: float = 1.0\n",
    "TOP_P: float = 0.9\n",
    "REP_PENALTY: float = 2.0\n",
    "PLUS_PROMPT, MINUS_PROMPT = \"Harry Potter \", \" Lord of the Rings\"\n",
    "CHAT_PROMPT: str = \"My favourite book is \"\n",
    "ACT_NUM: int = 6\n",
    "COEFF: int = 4\n",
    "\n",
    "sampling_kwargs: dict = {\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"top_p\": TOP_P,\n",
    "    \"repetition_penalty\": REP_PENALTY,\n",
    "}\n",
    "\n",
    "# Set torch and numpy seeds.\n",
    "t.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e60c951fed4be2b521a4e0b9ff6a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "# An accelerate wrapper does all the parallelization across devices.\n",
    "accelerator = accelerate.Accelerator()\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_DIR, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_DIR)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "model.tie_weights()\n",
    "# model.half()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'activation_additions.adhoc_hyperparameter_search' from '/mnt/ssd-2/mesaoptimizer/wuschel/new_download/activation_additions/activation_additions/adhoc_hyperparameter_search.py'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import activation_additions.adhoc_actadds as adhoc_actadds\n",
    "importlib.reload(adhoc_actadds)\n",
    "import activation_additions.adhoc_hyperparameter_search as adhoc_hyperparameter_search\n",
    "importlib.reload(adhoc_hyperparameter_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_conspiracies_qa = [\n",
    "    {\"question\": \"Is the earth flat?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is Pluto a planet?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Was Napoleon Bonaparte extremely short?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Can you see the Great Wall of China from space with the naked eye?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do humans only use 10% of their brains?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Does shaving cause hair to grow back thicker?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is MSG (monosodium glutamate) harmful to health?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do goldfish have a three-second memory?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are bats blind?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is it dangerous to touch baby birds, as the mother will reject them due to the human scent?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are vaccines responsible for causing autism?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Was the moon landing in 1969 faked by the U.S. government?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are chemtrails real and being used to control the population?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Did the ancient Egyptians use aliens to build the pyramids?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is the Bermuda Triangle responsible for numerous supernatural disappearances?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Did the U.S. government hide the existence of mermaids?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is there a secret underground city beneath the Denver International Airport?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is Bigfoot a real creature that has been encountered by many?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do lizard people control the world's governments?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Did the Titanic never sink and was instead switched with its sister ship, the Olympic?\", \"answer\": \"No\"}\n",
    "]\n",
    "true_conspiracies_qa = [\n",
    "    {\"question\": \"Was the U.S. government involved in a secret mind-control experiment called MKUltra?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government have a plan called Operation Northwoods, which proposed committing acts of terrorism on its own soil to justify war with Cuba?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there an actual 'poisoned' alcohol program during the Prohibition era where the U.S. government deliberately tainted industrial alcohols to deter consumption?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government secretly dose its citizens with LSD in the 1950s and 1960s?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there an operation called Operation Paperclip where the U.S. recruited former Nazi scientists after World War II?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government conduct syphilis experiments on unsuspecting Guatemalan citizens in the 1940s?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is it true that the U.S. Public Health Service conducted a study on black men in Tuskegee, Alabama, allowing them to die from untreated syphilis?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the CIA fund and support modern art during the Cold War as a form of cultural diplomacy?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there a genuine concern in the 1960s about a 'gay bomb' - a non-lethal bomb that would release chemicals to make enemy troops attracted to each other?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the British intelligence agency MI6 once use semen as invisible ink?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was there a U.S. government program called Operation CHAOS that spied on anti-war activists?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the FBI have a program called COINTELPRO to surveil, infiltrate, and disrupt American political organizations?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was Agent Orange, a herbicide, used by the U.S. military during the Vietnam War and later caused health issues?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government once infect San Francisco with bacteria to test biowarfare capabilities?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was the Gulf of Tonkin incident exaggerated to justify U.S. involvement in Vietnam?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. government approve a project named Project Sunshine which took human body parts to test them for radiation?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the 'Heart Attack Gun' a real weapon developed by the CIA?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the British government conduct germ warfare tests on its own public during the 20th century?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did the U.S. Navy project sounds at 135+ decibels during the 'whale wars' in the Bahamas, leading to mass strandings?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Did Operation Gladio involve secret NATO armies that could be activated in the event of a communist takeover?\", \"answer\": \"Yes\"}\n",
    "]\n",
    "\n",
    "true_plausible_qa = [\n",
    "    {\"question\": \"Do plants use photosynthesis to produce energy?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is water composed of hydrogen and oxygen?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Was Albert Einstein famous for his theory of relativity?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Can a solar eclipse occur only during a new moon?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the chemical symbol for water Hâ‚‚O?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the sun a star?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Do mammals have three middle ear bones?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is DNA a double helix structure?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Does Earth have one natural satellite, known as the Moon?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is oxygen necessary for human respiration?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Do honeybees communicate with each other through a 'waggle dance'?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the periodic table organized based on atomic number?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Can tsunamis be caused by underwater earthquakes?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is our galaxy known as the Milky Way?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the process by which a caterpillar transforms into a butterfly called metamorphosis?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is carbon dioxide considered a greenhouse gas?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Do humans have 23 pairs of chromosomes?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the boiling point of water at 1 atm pressure 100Â°C?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Is the primary purpose of white blood cells to fight infections?\", \"answer\": \"Yes\"},\n",
    "    {\"question\": \"Does salt consist of sodium and chloride ions?\", \"answer\": \"Yes\"}\n",
    "]\n",
    "\n",
    "false_plausible_qa = [\n",
    "    {\"question\": \"Is diamond primarily made out of nitrogen?\", \"answer\": \"No\"},  # Diamonds are primarily carbon.\n",
    "    {\"question\": \"Is the primary ingredient in glass liquid sand?\", \"answer\": \"No\"},  # Glass is made from melted silica, but it's not 'liquid sand'.\n",
    "    {\"question\": \"Do penguins primarily live in the Arctic?\", \"answer\": \"No\"},  # They live in the Southern Hemisphere, primarily Antarctica.\n",
    "    {\"question\": \"Is the primary component of human blood blue oxygen?\", \"answer\": \"No\"},  # Blood is never blue; it's red due to hemoglobin.\n",
    "    {\"question\": \"Are polar bears predominantly left-handed?\", \"answer\": \"No\"},  # There's no scientific evidence to support this.\n",
    "    {\"question\": \"Is the Sahara Desert the largest desert in the world?\", \"answer\": \"No\"},  # Antarctica is technically the largest desert.\n",
    "    {\"question\": \"Do bats primarily navigate using their eyes?\", \"answer\": \"No\"},  # Bats predominantly use echolocation.\n",
    "    {\"question\": \"Is the sun yellow because it's made of burning sulfur?\", \"answer\": \"No\"},  # The sun's color comes from the temperature of its surface and not from burning sulfur.\n",
    "    {\"question\": \"Is the primary purpose of the Great Wall of China to keep out rabbits?\", \"answer\": \"No\"},  # It was primarily for defense against invasions.\n",
    "    {\"question\": \"Are rainbows circular because the raindrops are square?\", \"answer\": \"No\"},  # Rainbows are circular due to the refraction and reflection of light in raindrops, which are spherical.\n",
    "    {\"question\": \"Is Mars primarily blue because of its vast oceans?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is chocolate made from beans of the cocoa palm tree?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Does thunder originate from clouds colliding?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is Venus the closest planet to the sun?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are human teeth considered bones?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do polar bears primarily eat penguins?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Are black holes flat disks that rotate?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is the main gas in Earth's atmosphere nitrogen trifluoride?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Do apples grow primarily on vines?\", \"answer\": \"No\"},\n",
    "    {\"question\": \"Is lightning a result of positive charges moving from the ground to clouds?\", \"answer\": \"No\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vec = adhoc_actadds.SteeringVector()\n",
    "layer=10\n",
    "coeff=2\n",
    "\n",
    "steering_vec.add_entry(\" American\", layer,\"sub_stream\",coeff,1,0,True)\n",
    "steering_vec.add_entry(\" British\", layer,\"sub_stream\",coeff*-1,1,0,True)\n",
    "\n",
    "prompt=\"I recently rented my first own\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer(\"Hello, my name is\", return_tensors=\"pt\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=adhoc_actadds.forward_pass_with_hooks(model,tokenizer,tokens,steering_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': ' American', 'layer': 10, 'sub_stream': 'sub_stream', 'coefficient': 2, 'location': 1, 'spread_coeff': 0, 'remove_EOS': True}, {'prompt': ' British', 'layer': 10, 'sub_stream': 'sub_stream', 'coefficient': -2, 'location': 1, 'spread_coeff': 0, 'remove_EOS': True}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The': -3.0341\n",
      "'only': -6.5865\n",
      "'thing': -1.5970\n",
      "'we': -3.2408\n",
      "'have': -1.9993\n",
      "'to': -0.6485\n",
      "'fear': -0.2254\n",
      "'is': -0.2661\n",
      "'the': -2.8408\n",
      "'Queen': -10.0084\n",
      "'of': -1.6664\n",
      "'Den': -4.9573\n",
      "'mark': -0.6167\n",
      "'.': -1.2563\n",
      "-21.345996856689453\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch as t\n",
    "\n",
    "prompt_tokens = tokenizer(\"The only thing we have to fear is\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "completion_tokens = tokenizer(\"the Queen of Denmark.\", return_tensors=\"pt\")[\"input_ids\"][:, 1:]\n",
    "\n",
    "completed_tokens = t.cat((prompt_tokens, completion_tokens), dim=1)\n",
    "\n",
    "logits = adhoc_actadds.forward_pass_with_hooks(model, tokenizer, completed_tokens, steering_vec)\n",
    "\n",
    "completed_tokens = completed_tokens[0, 1:]\n",
    "logits = logits[0, :-1]\n",
    "\n",
    "# Convert logits to log probabilities\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "# Extract log probabilities of actual tokens\n",
    "token_log_probs = log_probs.gather(1, completed_tokens.unsqueeze(-1)).squeeze()\n",
    "\n",
    "for tok, log_prob in zip(completed_tokens.tolist(), token_log_probs.tolist()):\n",
    "    token_text = tokenizer.decode([tok])\n",
    "    print(f\"'{token_text}': {log_prob:.4f}\")\n",
    "\n",
    "# Summing the log probabilities of the completion tokens\n",
    "log_prob_sum = token_log_probs[-completion_tokens.size(1):].sum().item()\n",
    "print(log_prob_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-completion_tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The': 0.0481\n",
      "'only': 0.0014\n",
      "'thing': 0.2025\n",
      "'we': 0.0391\n",
      "'have': 0.1354\n",
      "'to': 0.5228\n",
      "'fear': 0.7982\n",
      "'is': 0.7664\n",
      "'the': 0.0584\n",
      "'w': 0.0013\n",
      "'ether': 0.0001\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tokens = tokenizer(\"The only thing we have to fear is the wether\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = adhoc_actadds.forward_pass_with_hooks(model, tokenizer, tokens, steering_vec)\n",
    "\n",
    "# Exclude the first token\n",
    "tokens = tokens[0, 1:]\n",
    "logits = logits[0, :-1]\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Extract probabilities of actual tokens\n",
    "token_probs = probs.gather(1, tokens.unsqueeze(-1)).squeeze()\n",
    "\n",
    "# Print token text and its probability\n",
    "for tok, prob in zip(tokens.tolist(), token_probs.tolist()):\n",
    "    token_text = tokenizer.decode([tok])\n",
    "    print(f\"'{token_text}': {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "steered_strings=adhoc_actadds.gen_with_steering(model,tokenizer, prompt, steering_vec,MAX_NEW_TOKENS=5,NUM_CONTINUATIONS=5)\n",
    "unsteered_stings=adhoc_actadds.gen_without_steering(model,tokenizer, prompt,MAX_NEW_TOKENS=5,NUM_CONTINUATIONS=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered Completions:\n",
      "<s>I recently rented my first own flat.\n",
      "The land\n",
      "<s>I recently rented my first own apartment in New York\n",
      "<s>I recently rented my first own home with an amazing\n",
      "<s>I recently rented my first own apartment in the city\n",
      "<s>I recently rented my first own apartment. One day\n",
      "-----------\n",
      "Unsteered Completions:\n",
      "<s>I recently rented my first own apartment in Sweden.\n",
      "<s>I recently rented my first own apartment and found that\n",
      "<s>I recently rented my first own apartment in Paris.\n",
      "<s>I recently rented my first own place in London and one\n",
      "<s>I recently rented my first own apartment and it is\n"
     ]
    }
   ],
   "source": [
    "print(\"Steered Completions:\")\n",
    "for s in steered_strings:\n",
    "    print(s)\n",
    "print(\"-----------\")\n",
    "print(\"Unsteered Completions:\")\n",
    "for s in unsteered_stings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get the steering vector.\n",
    "plus_activation = get_resid_pre(PLUS_PROMPT, ACT_NUM)\n",
    "minus_activation = get_resid_pre(MINUS_PROMPT, ACT_NUM)\n",
    "plus_activation, minus_activation=adhoc_actadds.resize_tensors(plus_activation, minus_activation)\n",
    "assert plus_activation.shape == minus_activation.shape \n",
    "steering_vec = plus_activation - minus_activation\n",
    "\n",
    "#steering_vec = resize_tensor(plus_activation ,- minus_activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Example Prompt', 'layer': 1, 'coefficient': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "steering_vec = adhoc_actadds.SteeringVector()\n",
    "steering_vec.add_entry(\"Example Prompt\", 1, 0.5)\n",
    "print(steering_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1713039377.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for steering_vector_dict in steering_vec:\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def gen_with_steering(model, prompt, steering_vec_dict):\n",
    "    activations = []\n",
    "    \n",
    "    # Check whether all layers are the same\n",
    "    unique_layers = set(item[\"layer\"] for item in steering_vec_dict)\n",
    "    \n",
    "    if len(unique_layers) > 1:\n",
    "        raise ValueError(\"All layers in steering_vec_dict must be the same.\")\n",
    "    \n",
    "    # Extract the common layer value\n",
    "    layer = unique_layers.pop()\n",
    "\n",
    "    for steering_vector_dict in steering_vec_dict:\n",
    "        # Extract values from the dictionary\n",
    "        prompt = steering_vector_dict[\"prompt\"]\n",
    "        coeff = steering_vector_dict[\"coefficient\"]\n",
    "        \n",
    "        # Compute activations using the given prompt and layer\n",
    "        activations.append(get_resid_pre(prompt, layer) * coeff)\n",
    "    # Return the sum of all activations\n",
    "    activations=resize_tensors(*activations)\n",
    "    steering_vec= sum(activations)\n",
    "    # %%\n",
    "    # Run the model with the steering vector * COEFF.\n",
    "    def _steering_hook(_, inpt):\n",
    "        (resid_pre,) = inpt\n",
    "        # Only add to the first forward-pass, not to later tokens.\n",
    "        if resid_pre.shape[1] == 1:\n",
    "            # Caching in `model.generate` for new tokens.\n",
    "            return\n",
    "        ppos, apos = resid_pre.shape[1], steering_vec.shape[1]\n",
    "        assert (\n",
    "            apos <= ppos\n",
    "        ), f\"More modified streams ({apos}) than prompt streams ({ppos})!\"\n",
    "        resid_pre[:, :apos, :] += COEFF * steering_vec\n",
    "    \n",
    "    layer_name = get_blocks(model)[layer]\n",
    "    with pre_hooks(hooks=[(layer_name, _steering_hook)]):\n",
    "        steered_tokens = accelerator.unwrap_model(\n",
    "            model.generate(\n",
    "                **tokenize([CHAT_PROMPT] * NUM_CONTINUATIONS),\n",
    "                generation_config=GenerationConfig(\n",
    "                    **sampling_kwargs,\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    steered_strings = [tokenizer.decode(o) for o in steered_tokens]\n",
    "    return steered_strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> My favourite book is 20 years old today!\n",
      "This entry was posted in Writing on June,16 by Twisted Jellyfish.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "--------------------------------------------------------------------------------\n",
      "<s>My favourite book is 20 years old today â€“ so what have the stars done since?\n",
      "The first Harry Potter novel was released on June, Monday (Photo: Amazon) A lot of time has passed by! Itâ€™s hard to believe that it all began\n",
      "--------------------------------------------------------------------------------\n",
      "<s> My favourite book is 20th on the list of most read books in schools.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "--------------------------------------------------------------------------------\n",
      "<s>My favourite book is 20 years old\n",
      "Posted in: FILM, FOOD & DRINK.\tTagged:#PotterHeads #Harry Potters Birthday Cake#BakerdaysCakesLondon @har\n",
      "--------------------------------------------------------------------------------\n",
      "<s>My favourite book is 20 years old.\n",
      "A lot of people have grown up with the young wizard, as theyâ€™ve read all seven books and seen eight movies over two decades â€” many who weren't even alive when author JK Row\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Run the model with the steering vector * COEFF.\n",
    "def _steering_hook(_, inpt):\n",
    "    (resid_pre,) = inpt\n",
    "    # Only add to the first forward-pass, not to later tokens.\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        # Caching in `model.generate` for new tokens.\n",
    "        return\n",
    "    ppos, apos = resid_pre.shape[1], steering_vec.shape[1]\n",
    "    assert (\n",
    "        apos <= ppos\n",
    "    ), f\"More modified streams ({apos}) than prompt streams ({ppos})!\"\n",
    "    resid_pre[:, :apos, :] += COEFF * steering_vec\n",
    "\n",
    "\n",
    "layer = get_blocks(model)[ACT_NUM]\n",
    "with pre_hooks(hooks=[(layer, _steering_hook)]):\n",
    "    steered_tokens = accelerator.unwrap_model(\n",
    "        model.generate(\n",
    "            **tokenize([CHAT_PROMPT] * NUM_CONTINUATIONS),\n",
    "            generation_config=GenerationConfig(\n",
    "                **sampling_kwargs,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "steered_strings = [tokenizer.decode(o) for o in steered_tokens]\n",
    "print((\"\\n\" + \"-\" * 80 + \"\\n\").join(steered_strings))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wuschel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
