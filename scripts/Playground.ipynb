{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic demonstration of sweeps and metrics operation.\"\"\"\n",
    "\n",
    "# %%\n",
    "# Imports, etc.\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from activation_additions import (\n",
    "    prompt_utils,\n",
    "    utils,\n",
    "    metrics,\n",
    "    hook_utils,\n",
    "    hyperparameter_search\n",
    ")\n",
    "from activation_additions.prompt_utils import (\n",
    "    ActivationAddition,\n",
    "    pad_tokens_to_match_activation_additions,\n",
    "    get_block_name,\n",
    ")\n",
    "utils.enable_ipython_reload()\n",
    "\n",
    "# Disable gradients to save memory during inference\n",
    "_ = torch.set_grad_enabled(False)\n",
    "\n",
    "from typing import List, Union,Dict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load a model\n",
    "MODEL = HookedTransformer.from_pretrained(model_name=\"gpt2-xl\", device=\"cpu\")\n",
    "_ = MODEL.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=layer_coefficient_gridsearch(\n",
    "    model=MODEL,\n",
    "    prompts=[\"The Most beautyful city in the world is\"],\n",
    "    weighted_steering_prompts={\" Rome\":1,\" Paris\":-1},\n",
    "    Layer_list=list(range(6,12)),\n",
    "    coefficient_list=list(range(0,10,2)),\n",
    "    wanted_completions=\" Rome\",\n",
    "    unwanted_completions=\" Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21780490269884467"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "perplexity=conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The only thing we have to fear is\"),MODEL.to_tokens(\" fear itself\")[:, 1:])\n",
    "perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)\n",
    "#for act_name, hook_fn in hook_fns.items():\n",
    "#    MODEL.add_hook(act_name, hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hook_fn_from_activations.<locals>.prompt_hook() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n\u001b[1;32m      4\u001b[0m perplexity_1\u001b[39m=\u001b[39mhyperparameter_search\u001b[39m.\u001b[39mconditional_perplexity(MODEL,\n\u001b[1;32m      5\u001b[0m     MODEL\u001b[39m.\u001b[39mto_tokens(\u001b[39m\"\u001b[39m\u001b[39mThe primary emtion, I have towards you is\u001b[39m\u001b[39m\"\u001b[39m),MODEL\u001b[39m.\u001b[39mto_tokens(\u001b[39m\"\u001b[39m\u001b[39m hatred.\u001b[39m\u001b[39m\"\u001b[39m)[:, \u001b[39m1\u001b[39m:])\n\u001b[0;32m----> 6\u001b[0m perplexity_2\u001b[39m=\u001b[39mhyperparameter_search\u001b[39m.\u001b[39;49mconditional_perplexity(MODEL,\n\u001b[1;32m      7\u001b[0m     MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39mThe primary emtion, I have towards you is\u001b[39;49m\u001b[39m\"\u001b[39;49m),MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39m hatred.\u001b[39;49m\u001b[39m\"\u001b[39;49m)[:, \u001b[39m1\u001b[39;49m:],ActAds)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hyperparameter_search.py:47\u001b[0m, in \u001b[0;36mconditional_perplexity\u001b[0;34m(model, prompt_tokens, completion_tokens, ActAds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     metric_func\u001b[39m=\u001b[39mmetrics\u001b[39m.\u001b[39mget_logprob_metric(model, p_funcs\u001b[39m=\u001b[39m(\n\u001b[1;32m     43\u001b[0m                 partial(hook_utils\u001b[39m.\u001b[39madd_hooks_from_dict, hook_fns\u001b[39m=\u001b[39mhook_fns)\n\u001b[1;32m     44\u001b[0m                 ,\u001b[39mlambda\u001b[39;00m arg1, arg2: hook_utils\u001b[39m.\u001b[39mremove_and_return_hooks(arg1)\n\u001b[1;32m     45\u001b[0m             ))\n\u001b[1;32m     46\u001b[0m completed_tokens\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat((prompt_tokens, completion_tokens), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m metric\u001b[39m=\u001b[39mmetric_func([completed_tokens])\n\u001b[1;32m     48\u001b[0m completion_logprobs\u001b[39m=\u001b[39mmetric[\u001b[39m\"\u001b[39m\u001b[39mlogprob_actual_next_token\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39marray[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39mcompletion_tokens\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39msum\u001b[39m(completion_logprobs)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:186\u001b[0m, in \u001b[0;36mget_logprob_metric.<locals>.metric_func\u001b[0;34m(tokens_list, show_progress, index)\u001b[0m\n\u001b[1;32m    183\u001b[0m values_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m tqdm(tokens_list, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress):\n\u001b[1;32m    185\u001b[0m     \u001b[39m# Run the forward call on the (p) model\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     logits \u001b[39m=\u001b[39m forward_with_funcs(\n\u001b[1;32m    187\u001b[0m         model, p_funcs, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     values \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mactual_next_token\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m agg_mode:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# Logprob of the next token is just the negative of the\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# cross entropy loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:135\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         pre_ret \u001b[39m=\u001b[39m funcs[\u001b[39m0\u001b[39m](model)\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mfwd_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfwd_kwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "\u001b[0;31mTypeError\u001b[0m: hook_fn_from_activations.<locals>.prompt_hook() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=4, act_name=6,prompt=\" hate\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-4, act_name=6,prompt=\" love\")]\n",
    "MODEL.remove_all_hook_fns()\n",
    "perplexity_1=hyperparameter_search.conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The primary emtion, I have towards you is\"),MODEL.to_tokens(\" hatred.\")[:, 1:])\n",
    "perplexity_2=hyperparameter_search.conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The primary emtion, I have towards you is\"),MODEL.to_tokens(\" hatred.\")[:, 1:],ActAds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.645045101642609"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.645045101642609"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function activation_additions.hook_utils.hook_fn_from_activations.<locals>.prompt_hook(resid_pre: jaxtyping.Float[Tensor, 'batch pos d_model'], hook: Optional[transformer_lens.hook_points.HookPoint] = None) -> jaxtyping.Float[Tensor, 'batch pos d_model']>,\n",
       " <function activation_additions.hook_utils.hook_fn_from_activations.<locals>.prompt_hook(resid_pre: jaxtyping.Float[Tensor, 'batch pos d_model'], hook: Optional[transformer_lens.hook_points.HookPoint] = None) -> jaxtyping.Float[Tensor, 'batch pos d_model']>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook_fns[\"blocks.1.hook_resid_pre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_func=metrics.get_logprob_metric(MODEL, q_funcs=(\n",
    "                partial(hook_utils.add_hooks_from_dict, hook_fns=hook_fns),\n",
    "                hook_utils.remove_and_return_hooks\n",
    "             ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logprob_actual_next_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-2.6872368, -7.175348, -5.065762]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            logprob_actual_next_token\n",
       "0  [-2.6872368, -7.175348, -5.065762]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_func([MODEL.to_tokens(\"The end.\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=layer_coefficient_gridsearch(\n",
    "    model=MODEL,\n",
    "    prompts=[\"The Most beautyful city in the world is\"],\n",
    "    weighted_steering_prompts={\" Rome\":1,\" Paris\":-1},\n",
    "    Layer_list=list(range(6,12)),\n",
    "    coefficient_list=list(range(0,10,2)),\n",
    "    wanted_completions=\" Rome\",\n",
    "    unwanted_completions=\" Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[220]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conditional_perplexity(grid_df[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m],grid_df[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m],grid_df[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mconditional_perplexity\u001b[0;34m(model, prompt_tokens, completion_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m metric_func\u001b[39m=\u001b[39mmetrics\u001b[39m.\u001b[39mget_logprob_metric(model)\n\u001b[1;32m      3\u001b[0m completed_tokens\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat((prompt_tokens, completion_tokens), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m metric\u001b[39m=\u001b[39mmetric_func([completed_tokens])\n\u001b[1;32m      5\u001b[0m completion_logprobs\u001b[39m=\u001b[39mmetric[\u001b[39m\"\u001b[39m\u001b[39mlogprob_actual_next_token\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39marray[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39mcompletion_tokens\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39msum\u001b[39m(completion_logprobs)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:186\u001b[0m, in \u001b[0;36mget_logprob_metric.<locals>.metric_func\u001b[0;34m(tokens_list, show_progress, index)\u001b[0m\n\u001b[1;32m    183\u001b[0m values_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m tqdm(tokens_list, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress):\n\u001b[1;32m    185\u001b[0m     \u001b[39m# Run the forward call on the (p) model\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     logits \u001b[39m=\u001b[39m forward_with_funcs(\n\u001b[1;32m    187\u001b[0m         model, p_funcs, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     values \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mactual_next_token\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m agg_mode:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# Logprob of the next token is just the negative of the\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# cross entropy loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:135\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         pre_ret \u001b[39m=\u001b[39m funcs[\u001b[39m0\u001b[39m](model)\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mfwd_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfwd_kwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "conditional_perplexity(grid_df[0][0],grid_df[0][1],grid_df[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
