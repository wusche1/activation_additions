{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic demonstration of sweeps and metrics operation.\"\"\"\n",
    "\n",
    "# %%\n",
    "# Imports, etc.\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from activation_additions import (\n",
    "    prompt_utils,\n",
    "    utils,\n",
    "    metrics,\n",
    "    hook_utils,\n",
    "    hyperparameter_search\n",
    ")\n",
    "from activation_additions.prompt_utils import (\n",
    "    ActivationAddition,\n",
    "    pad_tokens_to_match_activation_additions,\n",
    "    get_block_name,\n",
    ")\n",
    "utils.enable_ipython_reload()\n",
    "\n",
    "# Disable gradients to save memory during inference\n",
    "_ = torch.set_grad_enabled(False)\n",
    "\n",
    "from typing import List, Union,Dict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from activation_additions import prompt_utils, hook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mactivation_additions\u001b[39;00m \u001b[39mimport\u001b[39;00m prompt_utils, hook_utils\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load a model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m MODEL \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt2-xl\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m _ \u001b[39m=\u001b[39m MODEL\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m ActAds \u001b[39m=\u001b[39m[prompt_utils\u001b[39m.\u001b[39mActivationAddition(coeff\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, act_name\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhi\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m          prompt_utils\u001b[39m.\u001b[39mActivationAddition(coeff\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, act_name\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbye\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:880\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_state_dict_to_device, tokenizer, move_to_device, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m         center_writing_weights \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39m# HookedTransformer parameter names.\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m state_dict \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39;49mget_pretrained_state_dict(\n\u001b[1;32m    881\u001b[0m     official_model_name, cfg, hf_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfrom_pretrained_kwargs\n\u001b[1;32m    882\u001b[0m )\n\u001b[1;32m    884\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m    885\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(cfg, tokenizer, move_to_device)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/loading_from_pretrained.py:938\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, cfg, hf_model, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         hf_model \u001b[39m=\u001b[39m BertForPreTraining\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    935\u001b[0m             official_model_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    936\u001b[0m         )\n\u001b[1;32m    937\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m         hf_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    939\u001b[0m             official_model_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    940\u001b[0m         )\n\u001b[1;32m    942\u001b[0m     \u001b[39m# Load model weights, and fold in layer norm weights\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m hf_model\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:484\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    485\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    488\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2675\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2672\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2674\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2675\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2677\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2678\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:965\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    964\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m GPT2Model(config)\n\u001b[1;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m     \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:683\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m--> 683\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    684\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    686\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:683\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m--> 683\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39;49mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    684\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    686\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:376\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m GPT2Attention(config, is_cross_attention\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, layer_idx\u001b[39m=\u001b[39mlayer_idx)\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_cross_attn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(hidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m--> 376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m GPT2MLP(inner_dim, config)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:349\u001b[0m, in \u001b[0;36mGPT2MLP.__init__\u001b[0;34m(self, intermediate_size, config)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    348\u001b[0m embed_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m--> 349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc \u001b[39m=\u001b[39m Conv1D(intermediate_size, embed_dim)\n\u001b[1;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj \u001b[39m=\u001b[39m Conv1D(embed_dim, intermediate_size)\n\u001b[1;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39m=\u001b[39m ACT2FN[config\u001b[39m.\u001b[39mactivation_function]\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformers/pytorch_utils.py:99\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[0;34m(self, nf, nx)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mempty(nx, nf))\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(nf))\n\u001b[0;32m---> 99\u001b[0m nn\u001b[39m.\u001b[39;49minit\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, std\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from activation_additions import prompt_utils, hook_utils\n",
    "# Load a model\n",
    "MODEL = HookedTransformer.from_pretrained(model_name=\"gpt2-xl\", device=\"cpu\")\n",
    "_ = MODEL.to(\"cuda:0\")\n",
    "\n",
    "\n",
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "ActAd=ActivationAddition(prompt=\"Test\", coeff=0, act_name=0)\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)\n",
    "\n",
    "tokens=MODEL.to_tokens(\"hallo Welt\")\n",
    "\n",
    "MODEL.remove_all_hook_fns()\n",
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "gpt2_logits, gpt2_cache = MODEL.run_with_cache(tokens, remove_batch_dim=True)\n",
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ActAd\u001b[39m=\u001b[39mprompt_utils\u001b[39m.\u001b[39mActivationAddition(prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m\"\u001b[39m, coeff\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, act_name\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m hook_fns\u001b[39m=\u001b[39mhook_utils\u001b[39m.\u001b[39;49mhook_fns_from_activation_additions(MODEL, [ActAd])\n\u001b[1;32m      4\u001b[0m tokens\u001b[39m=\u001b[39mMODEL\u001b[39m.\u001b[39mto_tokens(\u001b[39m\"\u001b[39m\u001b[39mhallo Welt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:307\u001b[0m, in \u001b[0;36mhook_fns_from_activation_additions\u001b[0;34m(model, activation_additions, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Takes a list of `ActivationAddition`s and makes a single activation-modifying forward hook.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m    added in.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# Get the activation dictionary\u001b[39;00m\n\u001b[1;32m    305\u001b[0m activation_dict: Dict[\n\u001b[1;32m    306\u001b[0m     \u001b[39mstr\u001b[39m, List[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m--> 307\u001b[0m ] \u001b[39m=\u001b[39m get_activation_dict(model, activation_additions)\n\u001b[1;32m    309\u001b[0m \u001b[39m# Make the hook functions\u001b[39;00m\n\u001b[1;32m    310\u001b[0m hook_fns: Dict[\u001b[39mstr\u001b[39m, List[Callable]] \u001b[39m=\u001b[39m hook_fns_from_act_dict(\n\u001b[1;32m    311\u001b[0m     activation_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:59\u001b[0m, in \u001b[0;36mget_activation_dict\u001b[0;34m(model, activation_additions)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Add activations for each prompt\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m activation_addition \u001b[39min\u001b[39;00m activation_additions:\n\u001b[1;32m     58\u001b[0m     activation_dict[activation_addition\u001b[39m.\u001b[39mact_name]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> 59\u001b[0m         get_prompt_activations(model, activation_addition)\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m activation_dict\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:36\u001b[0m, in \u001b[0;36mget_prompt_activations\u001b[0;34m(model, activation_addition)\u001b[0m\n\u001b[1;32m     32\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(activation_addition\u001b[39m.\u001b[39mprompt)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Run the forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# ActivationCache is basically Dict[str, torch.Tensor]\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m cache: ActivationCache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m     37\u001b[0m     tokens,\n\u001b[1;32m     38\u001b[0m     names_filter\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m act_name: act_name \u001b[39m==\u001b[39;49m activation_addition\u001b[39m.\u001b[39;49mact_name,\n\u001b[1;32m     39\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[39m# Return cached activations times coefficient\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m activation_addition\u001b[39m.\u001b[39mcoeff \u001b[39m*\u001b[39m cache[activation_addition\u001b[39m.\u001b[39mact_name]\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "ActAd=prompt_utils.ActivationAddition(prompt=\"Test\", coeff=0, act_name=0)\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, [ActAd])\n",
    "\n",
    "tokens=MODEL.to_tokens(\"hallo Welt\")\n",
    "\n",
    "MODEL.remove_all_hook_fns()\n",
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "gpt2_logits, gpt2_cache = MODEL.run_with_cache(tokens, remove_batch_dim=True)\n",
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m act_name, hook_fn \u001b[39min\u001b[39;00m hook_fns\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     MODEL\u001b[39m.\u001b[39madd_hook(act_name, hook_fn)\n\u001b[0;32m---> 10\u001b[0m gpt2_logits, gpt2_cache \u001b[39m=\u001b[39m MODEL\u001b[39m.\u001b[39;49mrun_with_cache(tokens, remove_batch_dim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     11\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)\n",
    "\n",
    "tokens=MODEL.to_tokens(\"hallo Welt\")\n",
    "\n",
    "MODEL.remove_all_hook_fns()\n",
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "gpt2_logits, gpt2_cache = MODEL.run_with_cache(tokens, remove_batch_dim=True)\n",
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=layer_coefficient_gridsearch(\n",
    "    model=MODEL,\n",
    "    prompts=[\"The Most beautyful city in the world is\"],\n",
    "    weighted_steering_prompts={\" Rome\":1,\" Paris\":-1},\n",
    "    Layer_list=list(range(6,12)),\n",
    "    coefficient_list=list(range(0,10,2)),\n",
    "    wanted_completions=\" Rome\",\n",
    "    unwanted_completions=\" Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21780490269884467"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "perplexity=conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The only thing we have to fear is\"),MODEL.to_tokens(\" fear itself\")[:, 1:])\n",
    "perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)\n",
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.1.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "for act_name, hook_fn in hook_fns.items():\n",
    "    print(act_name)\n",
    "    MODEL.add_hook(act_name, hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m act_name, hook_fn \u001b[39min\u001b[39;00m hook_fns\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     MODEL\u001b[39m.\u001b[39madd_hook(act_name, hook_fn)\n\u001b[0;32m----> 4\u001b[0m gpt2_logits, gpt2_cache \u001b[39m=\u001b[39m MODEL\u001b[39m.\u001b[39;49mrun_with_cache(tokens, remove_batch_dim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "MODEL.remove_all_hook_fns()\n",
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "gpt2_logits, gpt2_cache = MODEL.run_with_cache(tokens, remove_batch_dim=True)\n",
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HookedTransformer' object has no attribute 'hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n\u001b[0;32m----> 2\u001b[0m MODEL\u001b[39m.\u001b[39;49mhook\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HookedTransformer' object has no attribute 'hook'"
     ]
    }
   ],
   "source": [
    "MODEL.remove_all_hook_fns()\n",
    "MODEL.hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fdc8a6fa6845fe9e222621b963b95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99383d45c3a471ca79694b81380c320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m act_name, hook_fn \u001b[39min\u001b[39;00m hook_fns\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     MODEL\u001b[39m.\u001b[39madd_hook(act_name, hook_fn)\n\u001b[0;32m----> 8\u001b[0m MODEL\u001b[39m.\u001b[39;49mgenerate(MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39mThe only thing we have to fear is\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1483\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, num_return_sequences, use_past_kv_cache, prepend_bos, return_type, verbose)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\n\u001b[1;32m   1478\u001b[0m             tokens[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:],\n\u001b[1;32m   1479\u001b[0m             return_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1480\u001b[0m             past_kv_cache\u001b[39m=\u001b[39mpast_kv_cache,\n\u001b[1;32m   1481\u001b[0m         )\n\u001b[1;32m   1482\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1483\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m   1484\u001b[0m             tokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m, past_kv_cache\u001b[39m=\u001b[39;49mpast_kv_cache\n\u001b[1;32m   1485\u001b[0m         )\n\u001b[1;32m   1487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1488\u001b[0m     \u001b[39m# We input the entire sequence, as a [batch, pos] tensor, since we aren't using the cache\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(tokens, return_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "MODEL.remove_all_hook_fns()\n",
    "MODEL.generate(MODEL.to_tokens(\"The only thing we have to fear is\"))\n",
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)\n",
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "MODEL.generate(MODEL.to_tokens(\"The only thing we have to fear is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hook_fn_from_activations() got an unexpected keyword argument 'xvec_position'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n\u001b[0;32m----> 2\u001b[0m hook_utils\u001b[39m.\u001b[39;49mforward_with_activation_additions(MODEL,ActAds,\u001b[39m\"\u001b[39;49m\u001b[39mHallo Welt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:369\u001b[0m, in \u001b[0;36mforward_with_activation_additions\u001b[0;34m(model, activation_additions, input, xvec_position, injection_mode, **forward_kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     (\n\u001b[1;32m    362\u001b[0m         input_tokens,\n\u001b[1;32m    363\u001b[0m         activation_addition_len,\n\u001b[1;32m    364\u001b[0m     ) \u001b[39m=\u001b[39m pad_tokens_to_match_activation_additions(\n\u001b[1;32m    365\u001b[0m         model, input_tokens, activation_additions\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m \u001b[39m# TODO: TransformerLens now has a hooks() context manager, should\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m# move to latest version and use that to simplify this code\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m hook_fns \u001b[39m=\u001b[39m hook_fns_from_activation_additions(\n\u001b[1;32m    370\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    371\u001b[0m     activation_additions\u001b[39m=\u001b[39;49mactivation_additions,\n\u001b[1;32m    372\u001b[0m     xvec_position\u001b[39m=\u001b[39;49mxvec_position,\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    374\u001b[0m model\u001b[39m.\u001b[39mremove_all_hook_fns()\n\u001b[1;32m    375\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:310\u001b[0m, in \u001b[0;36mhook_fns_from_activation_additions\u001b[0;34m(model, activation_additions, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m activation_dict: Dict[\n\u001b[1;32m    306\u001b[0m     \u001b[39mstr\u001b[39m, List[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m    307\u001b[0m ] \u001b[39m=\u001b[39m get_activation_dict(model, activation_additions)\n\u001b[1;32m    309\u001b[0m \u001b[39m# Make the hook functions\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m hook_fns: Dict[\u001b[39mstr\u001b[39m, List[Callable]] \u001b[39m=\u001b[39m hook_fns_from_act_dict(\n\u001b[1;32m    311\u001b[0m     activation_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    312\u001b[0m )\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m hook_fns\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:276\u001b[0m, in \u001b[0;36mhook_fns_from_act_dict\u001b[0;34m(activation_dict, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m# Add hook functions for each activation name\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mfor\u001b[39;00m act_name, act_list \u001b[39min\u001b[39;00m activation_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    275\u001b[0m     \u001b[39m# Compose the hook functions for each prompt\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     act_fns: List[Callable] \u001b[39m=\u001b[39m [\n\u001b[1;32m    277\u001b[0m         hook_fn_from_activations(activations, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    278\u001b[0m         \u001b[39mfor\u001b[39;00m activations \u001b[39min\u001b[39;00m act_list\n\u001b[1;32m    279\u001b[0m     ]\n\u001b[1;32m    280\u001b[0m     hook_fns[act_name] \u001b[39m=\u001b[39m act_fns\n\u001b[1;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m hook_fns\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hook_utils.py:277\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m# Add hook functions for each activation name\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mfor\u001b[39;00m act_name, act_list \u001b[39min\u001b[39;00m activation_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    275\u001b[0m     \u001b[39m# Compose the hook functions for each prompt\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     act_fns: List[Callable] \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 277\u001b[0m         hook_fn_from_activations(activations, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    278\u001b[0m         \u001b[39mfor\u001b[39;00m activations \u001b[39min\u001b[39;00m act_list\n\u001b[1;32m    279\u001b[0m     ]\n\u001b[1;32m    280\u001b[0m     hook_fns[act_name] \u001b[39m=\u001b[39m act_fns\n\u001b[1;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m hook_fns\n",
      "\u001b[0;31mTypeError\u001b[0m: hook_fn_from_activations() got an unexpected keyword argument 'xvec_position'"
     ]
    }
   ],
   "source": [
    "MODEL.remove_all_hook_fns()\n",
    "hook_utils.forward_with_activation_additions(MODEL,ActAds,\"Hallo Welt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.8709, 13.0157, 10.4153,  ..., -2.5404, -0.8197,  7.0042],\n",
       "         [ 1.6135,  2.0764,  0.2411,  ..., -0.6971, -1.6178,  2.4038],\n",
       "         [ 9.9495,  7.5410,  3.5298,  ..., -2.5723,  0.2626,  7.0593],\n",
       "         [ 5.9748,  5.5619,  5.0105,  ..., -3.9446, -1.0525, 11.2003]]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.remove_all_hook_fns()\n",
    "MODEL.forward(MODEL.to_tokens(\"The end.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens\u001b[39m=\u001b[39mMODEL\u001b[39m.\u001b[39mto_tokens(\u001b[39m\"\u001b[39m\u001b[39mThe only thing we have to fear is fear itself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m forward\u001b[39m=\u001b[39mMODEL\u001b[39m.\u001b[39;49mforward(tokens)\n\u001b[1;32m      3\u001b[0m logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(forward, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m [logprob[truth]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m truth, logprob \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tokens[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m:], logprobs[\u001b[39m0\u001b[39m,:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])]\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "tokens=MODEL.to_tokens(\"The only thing we have to fear is fear itself\")\n",
    "forward=MODEL.forward(tokens)\n",
    "logprobs = torch.nn.functional.log_softmax(forward, dim=-1)\n",
    "\n",
    "[logprob[truth].item() for truth, logprob in zip(tokens[0,1:], logprobs[0,:,:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "The\n",
      "Log-probability of the correct next token: -2.6872360706329346\n",
      "---------------\n",
      " only\n",
      " first\n",
      "Log-probability of the correct next token: -6.218729496002197\n",
      "---------------\n",
      " thing\n",
      " thing\n",
      "Log-probability of the correct next token: -1.752655267715454\n",
      "---------------\n",
      " we\n",
      " that\n",
      "Log-probability of the correct next token: -3.370784282684326\n",
      "---------------\n",
      " have\n",
      " can\n",
      "Log-probability of the correct next token: -2.149301052093506\n",
      "---------------\n",
      " to\n",
      " to\n",
      "Log-probability of the correct next token: -0.559380054473877\n",
      "---------------\n",
      " fear\n",
      " fear\n",
      "Log-probability of the correct next token: -1.5785598754882812\n",
      "---------------\n",
      " is\n",
      " is\n",
      "Log-probability of the correct next token: -0.26736363768577576\n",
      "---------------\n",
      " fear\n",
      " fear\n",
      "Log-probability of the correct next token: -0.21575279533863068\n",
      "---------------\n",
      " itself\n",
      " itself\n",
      "Log-probability of the correct next token: -0.002052107360213995\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for truth, prediction, logprob in zip(tokens[0,1:], forward[0,:,:-1], logprobs[0,:,:-1]):\n",
    "    print(MODEL.tokenizer.decode(truth))\n",
    "    print(MODEL.tokenizer.decode(torch.argmax(prediction)))\n",
    "    print(\"Log-probability of the correct next token:\", logprob[truth].item())\n",
    "    print(\"---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_func=metrics.get_logprob_metric(MODEL)\n",
    "metric_func([MODEL.to_tokens(\"The end.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metric_func([MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39mThe end.\u001b[39;49m\u001b[39m\"\u001b[39;49m)])\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:186\u001b[0m, in \u001b[0;36mget_logprob_metric.<locals>.metric_func\u001b[0;34m(tokens_list, show_progress, index)\u001b[0m\n\u001b[1;32m    183\u001b[0m values_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m tqdm(tokens_list, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress):\n\u001b[1;32m    185\u001b[0m     \u001b[39m# Run the forward call on the (p) model\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     logits \u001b[39m=\u001b[39m forward_with_funcs(\n\u001b[1;32m    187\u001b[0m         model, p_funcs, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     values \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mactual_next_token\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m agg_mode:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# Logprob of the next token is just the negative of the\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# cross entropy loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:135\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         pre_ret \u001b[39m=\u001b[39m funcs[\u001b[39m0\u001b[39m](model)\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mfwd_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfwd_kwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "metric_func([MODEL.to_tokens(\"The end.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.1.hook_resid_pre\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     MODEL\u001b[39m.\u001b[39madd_hook(act_name, hook_fn)\n\u001b[1;32m      4\u001b[0m metric_func\u001b[39m=\u001b[39mmetrics\u001b[39m.\u001b[39mget_logprob_metric(MODEL)\n\u001b[0;32m----> 5\u001b[0m metric_func([MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39mThe end.\u001b[39;49m\u001b[39m\"\u001b[39;49m)])\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:186\u001b[0m, in \u001b[0;36mget_logprob_metric.<locals>.metric_func\u001b[0;34m(tokens_list, show_progress, index)\u001b[0m\n\u001b[1;32m    183\u001b[0m values_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m tqdm(tokens_list, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress):\n\u001b[1;32m    185\u001b[0m     \u001b[39m# Run the forward call on the (p) model\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     logits \u001b[39m=\u001b[39m forward_with_funcs(\n\u001b[1;32m    187\u001b[0m         model, p_funcs, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     values \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mactual_next_token\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m agg_mode:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# Logprob of the next token is just the negative of the\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# cross entropy loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:135\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         pre_ret \u001b[39m=\u001b[39m funcs[\u001b[39m0\u001b[39m](model)\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mfwd_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfwd_kwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "for act_name, hook_fn in hook_fns.items():\n",
    "    print(act_name)\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "metric_func=metrics.get_logprob_metric(MODEL)\n",
    "metric_func([MODEL.to_tokens(\"The end.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'functools.partial' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:133\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m0\u001b[39;49m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         pre_ret \u001b[39m=\u001b[39m funcs[\u001b[39m0\u001b[39m](model)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'functools.partial' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m MODEL\u001b[39m.\u001b[39mremove_all_hook_fns()\n\u001b[1;32m      4\u001b[0m perplexity_1\u001b[39m=\u001b[39mhyperparameter_search\u001b[39m.\u001b[39mconditional_perplexity(MODEL,\n\u001b[1;32m      5\u001b[0m     MODEL\u001b[39m.\u001b[39mto_tokens(\u001b[39m\"\u001b[39m\u001b[39mThe primary emtion, I have towards you is\u001b[39m\u001b[39m\"\u001b[39m),MODEL\u001b[39m.\u001b[39mto_tokens(\u001b[39m\"\u001b[39m\u001b[39m hatred.\u001b[39m\u001b[39m\"\u001b[39m)[:, \u001b[39m1\u001b[39m:])\n\u001b[0;32m----> 6\u001b[0m perplexity_2\u001b[39m=\u001b[39mhyperparameter_search\u001b[39m.\u001b[39;49mconditional_perplexity(MODEL,\n\u001b[1;32m      7\u001b[0m     MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39mThe primary emtion, I have towards you is\u001b[39;49m\u001b[39m\"\u001b[39;49m),MODEL\u001b[39m.\u001b[39;49mto_tokens(\u001b[39m\"\u001b[39;49m\u001b[39m hatred.\u001b[39;49m\u001b[39m\"\u001b[39;49m)[:, \u001b[39m1\u001b[39;49m:],ActAds)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/hyperparameter_search.py:47\u001b[0m, in \u001b[0;36mconditional_perplexity\u001b[0;34m(model, prompt_tokens, completion_tokens, ActAds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     metric_func\u001b[39m=\u001b[39mmetrics\u001b[39m.\u001b[39mget_logprob_metric(model, p_funcs\u001b[39m=\u001b[39m(\n\u001b[1;32m     43\u001b[0m                 partial(hook_utils\u001b[39m.\u001b[39madd_hooks_from_dict, hook_fns\u001b[39m=\u001b[39mhook_fns)\n\u001b[1;32m     44\u001b[0m                 \u001b[39m#,lambda arg1, arg2: hook_utils.remove_and_return_hooks(arg1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m             ))\n\u001b[1;32m     46\u001b[0m completed_tokens\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat((prompt_tokens, completion_tokens), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m metric\u001b[39m=\u001b[39mmetric_func([completed_tokens])\n\u001b[1;32m     48\u001b[0m completion_logprobs\u001b[39m=\u001b[39mmetric[\u001b[39m\"\u001b[39m\u001b[39mlogprob_actual_next_token\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39marray[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39mcompletion_tokens\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39msum\u001b[39m(completion_logprobs)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:186\u001b[0m, in \u001b[0;36mget_logprob_metric.<locals>.metric_func\u001b[0;34m(tokens_list, show_progress, index)\u001b[0m\n\u001b[1;32m    183\u001b[0m values_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m tqdm(tokens_list, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress):\n\u001b[1;32m    185\u001b[0m     \u001b[39m# Run the forward call on the (p) model\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     logits \u001b[39m=\u001b[39m forward_with_funcs(\n\u001b[1;32m    187\u001b[0m         model, p_funcs, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     values \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mactual_next_token\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m agg_mode:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# Logprob of the next token is just the negative of the\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# cross entropy loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:137\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39mfwd_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfwd_kwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m1\u001b[39;49m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         funcs[\u001b[39m1\u001b[39m](model, pre_ret)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'functools.partial' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=4, act_name=6,prompt=\" hate\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-4, act_name=6,prompt=\" love\")]\n",
    "MODEL.remove_all_hook_fns()\n",
    "perplexity_1=hyperparameter_search.conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The primary emtion, I have towards you is\"),MODEL.to_tokens(\" hatred.\")[:, 1:])\n",
    "perplexity_2=hyperparameter_search.conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The primary emtion, I have towards you is\"),MODEL.to_tokens(\" hatred.\")[:, 1:],ActAds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.645045101642609"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.645045101642609"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function activation_additions.hook_utils.hook_fn_from_activations.<locals>.prompt_hook(resid_pre: jaxtyping.Float[Tensor, 'batch pos d_model'], hook: Optional[transformer_lens.hook_points.HookPoint] = None) -> jaxtyping.Float[Tensor, 'batch pos d_model']>,\n",
       " <function activation_additions.hook_utils.hook_fn_from_activations.<locals>.prompt_hook(resid_pre: jaxtyping.Float[Tensor, 'batch pos d_model'], hook: Optional[transformer_lens.hook_points.HookPoint] = None) -> jaxtyping.Float[Tensor, 'batch pos d_model']>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook_fns[\"blocks.1.hook_resid_pre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hook_fns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m metric_func\u001b[39m=\u001b[39mmetrics\u001b[39m.\u001b[39mget_logprob_metric(MODEL, q_funcs\u001b[39m=\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m                 partial(hook_utils\u001b[39m.\u001b[39madd_hooks_from_dict, hook_fns\u001b[39m=\u001b[39mhook_fns),\n\u001b[1;32m      3\u001b[0m                 hook_utils\u001b[39m.\u001b[39mremove_and_return_hooks\n\u001b[1;32m      4\u001b[0m              ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hook_fns' is not defined"
     ]
    }
   ],
   "source": [
    "metric_func=metrics.get_logprob_metric(MODEL, q_funcs=(\n",
    "                partial(hook_utils.add_hooks_from_dict, hook_fns=hook_fns),\n",
    "                hook_utils.remove_and_return_hooks\n",
    "             ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logprob_actual_next_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-2.6872368, -7.175348, -5.065762]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            logprob_actual_next_token\n",
       "0  [-2.6872368, -7.175348, -5.065762]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_func([MODEL.to_tokens(\"The end.\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=layer_coefficient_gridsearch(\n",
    "    model=MODEL,\n",
    "    prompts=[\"The Most beautyful city in the world is\"],\n",
    "    weighted_steering_prompts={\" Rome\":1,\" Paris\":-1},\n",
    "    Layer_list=list(range(6,12)),\n",
    "    coefficient_list=list(range(0,10,2)),\n",
    "    wanted_completions=\" Rome\",\n",
    "    unwanted_completions=\" Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[220]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conditional_perplexity(grid_df[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m],grid_df[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m],grid_df[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mconditional_perplexity\u001b[0;34m(model, prompt_tokens, completion_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m metric_func\u001b[39m=\u001b[39mmetrics\u001b[39m.\u001b[39mget_logprob_metric(model)\n\u001b[1;32m      3\u001b[0m completed_tokens\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat((prompt_tokens, completion_tokens), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m metric\u001b[39m=\u001b[39mmetric_func([completed_tokens])\n\u001b[1;32m      5\u001b[0m completion_logprobs\u001b[39m=\u001b[39mmetric[\u001b[39m\"\u001b[39m\u001b[39mlogprob_actual_next_token\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39marray[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39mcompletion_tokens\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39msum\u001b[39m(completion_logprobs)\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:186\u001b[0m, in \u001b[0;36mget_logprob_metric.<locals>.metric_func\u001b[0;34m(tokens_list, show_progress, index)\u001b[0m\n\u001b[1;32m    183\u001b[0m values_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m tqdm(tokens_list, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress):\n\u001b[1;32m    185\u001b[0m     \u001b[39m# Run the forward call on the (p) model\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     logits \u001b[39m=\u001b[39m forward_with_funcs(\n\u001b[1;32m    187\u001b[0m         model, p_funcs, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     values \u001b[39m=\u001b[39m {}\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mactual_next_token\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m agg_mode:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# Logprob of the next token is just the negative of the\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# cross entropy loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/wuschel/activation_additions/activation_additions/metrics.py:135\u001b[0m, in \u001b[0;36mforward_with_funcs\u001b[0;34m(model, funcs, *fwd_args, **fwd_kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         pre_ret \u001b[39m=\u001b[39m funcs[\u001b[39m0\u001b[39m](model)\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mfwd_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfwd_kwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m funcs[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/mnt/ssd-2/mesaoptimizer/miniconda3/envs/wuschel_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "conditional_perplexity(grid_df[0][0],grid_df[0][1],grid_df[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
